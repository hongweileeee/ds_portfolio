{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Capstone Project - Zoom Ally: A Running Form Classifier (Overstriding)\n",
    "\n",
    "> Author: Lee Hongwei\n",
    "---\n",
    "\n",
    "## Problem Statement:\n",
    "\n",
    "Our challenge is to devise a solution that empowers individuals to **detect one of the most prevalent running form errors: overstriding**, attempting to strengthen Nike Run Club's position as a trusted virtual running coach and in turn, the app's ability to attract and retain users.\n",
    "\n",
    "---\n",
    "There are a total of ___ notebooks for this project:  \n",
    " 1. `01_Data_Collection_Video_Dl.ipynb`   \n",
    " 2. `02_Data_Collection_Pose_Estimation.ipynb`   \n",
    " 3. `03_Feature_Engineering_and_EDA.ipynb`\n",
    " 4. `04_Data_Preprocessing_and_Modelling.ipynb`\n",
    " \n",
    "**This Notebook:**\n",
    "- Extract pose landmark data from keypoints using mediapipe's pose estimation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for computer vision\n",
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for basic necessities\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Defining File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the filepaths for videos\n",
    "\n",
    "SOURCE_PATH_G = '../datasets/good_running' # Source path for no_overstride videos\n",
    "SOURCE_PATH_B = '../datasets/bad_running' # Source path for overstride videos\n",
    "\n",
    "# Get a list of all video files in the source folder\n",
    "video_files_g = [f for f in os.listdir(SOURCE_PATH_G) if f.endswith('.mp4')]\n",
    "video_files_b = [f for f in os.listdir(SOURCE_PATH_B) if f.endswith('.mp4')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pose Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/3j8BPdc.png' style = 'height:300px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediapipe collects 33 keypoints from humans detected in a video, with the indices of each keypoint marked as above. Each keypoint contains x, y, z and visibility coordinates within the video and is scaled between 0 to 1. Origin of coordinates in a video start from the top left corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Extracting and Drawing Keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Instantiate Model and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715063173.596034       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate holistic detection model\n",
    "mp_holistic = mp.solutions.holistic # Makes our keypoint detections\n",
    "mp_drawing = mp.solutions.drawing_utils # Draws the points and lines between points\n",
    "\n",
    "# Set mediapipe holistic model to detection and tracking confidence as 0.5 to allow easier tracking and fewer missing values.\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that passes in an image and the detection model that it will be used on\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    # Colour conversion as opencv reads in bgr\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Make image non-writeable to prevent accidental modification of the image\n",
    "    image.flags.writeable = False\n",
    "    # Image data is fixed and can be used to create prediction results of keypoint detections\n",
    "    results = model.process(image)\n",
    "    # Make image writeable again\n",
    "    image.flags.writeable = True\n",
    "    # Convert colour back to bgr for it to be readable by opencv\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Return 2 values: image and results. \n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that draws the landmarks from the 'pose_landmarks' function\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, \n",
    "                              results.pose_landmarks, # Note: Mediapipe has many other landmark markers such as face and hands which I will not be using\n",
    "                              mp_holistic.POSE_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Testing with webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an opencv object from our video\n",
    "cap = cv2.VideoCapture(0)  # Passing 0 into VideoCapture == Use WebCam footage (MacOS)\n",
    "\n",
    "# Set mediapipe model\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read feed\n",
    "    ret, frame = cap.read()\n",
    "    # Once feed is at its last frame, there will be no ret value, break loop and move to next video\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Make detections using previously defined function to detect keypoints\n",
    "    image, results = mediapipe_detection(frame, holistic)\n",
    "    print(results)\n",
    "\n",
    "    # Show to screen with defined function to draw points and lines\n",
    "    draw_landmarks(image, results)\n",
    "    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "    # Break loop if 'q' is pressed\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and destroy OpenCV windows\n",
    "cap.release()\n",
    "if cv2.getWindowProperty('OpenCV Feed', cv2.WND_PROP_VISIBLE) >= 1:\n",
    "    cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we can opt between extracting the data into a numpy array or a pandas dataframe/csv file. \n",
    "\n",
    "We opted to save each frame's data in numpy arrays for the following reasons:\n",
    "1. Lighter file size\n",
    "2. Ease of tracking skipped frames (mediapipe sometimes skips the frame if the pose estimation was not able to extract the data fast enough)\n",
    "3. Ease of conversion back into a pandas dataframe\n",
    "\n",
    "Cons of doing so:\n",
    "At this stage, we're not able to visualise the data with ease which a pandas dataframe offers with column names available. However since we're just extracting data, as long as the pose estimation model runs smoothly and in a controlled manner, we can accurately convert the numpy array into a pandas dataframe with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50638884  0.49915484 -0.61960572  0.99748868]\n",
      " [ 0.52479327  0.43191147 -0.56306452  0.99831372]\n",
      " [ 0.53690493  0.43197474 -0.56319326  0.9976967 ]\n",
      " [ 0.54576963  0.43299702 -0.56347585  0.99803662]\n",
      " [ 0.47238716  0.43674192 -0.56225872  0.99813634]]\n"
     ]
    }
   ],
   "source": [
    "# Extracting webcam results into a numpy array\n",
    "\n",
    "landmarks = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark])\n",
    "print(landmarks[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data extraction from keypoints as a function\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    landmarks = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark])\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Applying Pose Detection on Exported Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 No Overstride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for exported data (numpy arrays)\n",
    "EXPORT_PATH = '../datasets/no_overstride_npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F_Running_3.mp4', 'F_Running_2.mp4', 'F_Running_6.mp4', 'F_Running_7.mp4', 'F_Running_5.mp4', 'F_Running_4.mp4', 'M_Running_8.mp4', 'M_Running_3.mp4', 'M_Running_2.mp4', 'M_Running_1.mp4', 'M_Running_5.mp4', 'M_Running_4.mp4', 'M_Running_6.mp4', 'M_Running_7.mp4', 'F_Running_8.mp4']\n",
      "\n",
      "\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Checking files\n",
    "print(video_files_g)\n",
    "print('\\n')\n",
    "print(len(video_files_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SolutionBase.close of <mediapipe.python.solutions.holistic.Holistic object at 0x2962c44f0>>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop through video files for each .mp4 file\n",
    "for videomp4 in video_files_g:\n",
    "    # Construct the full path to the video file\n",
    "    vid_file = os.path.join(SOURCE_PATH_G, videomp4)\n",
    "    \n",
    "    # Create the directory to store the numpy arrays for this video\n",
    "    video_dir = os.path.join(EXPORT_PATH, videomp4[:-4])  # Remove the '.mp4' extension from the video file name\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    \n",
    "    # Create an opencv object from our video\n",
    "    cap = cv2.VideoCapture(vid_file)\n",
    "    \n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Timestep\n",
    "    timestep = 0\n",
    "\n",
    "    # Set mediapipe model\n",
    "\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        # Once feed is at its last frame, there will be no ret value, break loop and move to next video\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Make detections using previously defined function to detect keypoints\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # Export Keypoints\n",
    "        keypoints = extract_keypoints(results) # Extract keypoints from the results object\n",
    "\n",
    "        # Save numpy in new folder\n",
    "        npy_path = os.path.join(video_dir, str(timestep) + '.npy') # Create a path to save the numpy array to be generated below\n",
    "        np.save(npy_path, keypoints) # Save the keypoints data in a numpy array onto the path defined\n",
    "        \n",
    "        # Show to screen with defined function to draw points and lines\n",
    "        draw_landmarks(image, results)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Increase timestep by 1 for next frame\n",
    "        timestep += 1\n",
    "\n",
    "        # Break loop if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release video capture and destroy OpenCV windows\n",
    "cap.release()\n",
    "if cv2.getWindowProperty('OpenCV Feed', cv2.WND_PROP_VISIBLE) >= 1:\n",
    "    cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)  # Added this\n",
    "cap.release\n",
    "\n",
    "holistic.close\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Overstriding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for exported data (numpy arrays)\n",
    "DATA_PATH = '../datasets/overstride_npy'\n",
    "\n",
    "# Create video extraction source path \n",
    "source_path = '../datasets/bad_running/'\n",
    "\n",
    "# Get a list of all video files in the source folder\n",
    "video_files = [f for f in os.listdir(source_path) if f.endswith('.mp4')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Overstride_16.mp4',\n",
       " 'Overstride_17.mp4',\n",
       " 'Overstride_15.mp4',\n",
       " 'Overstride_14.mp4',\n",
       " 'Overstride_10.mp4',\n",
       " 'Overstride_11.mp4',\n",
       " 'Overstride_13.mp4',\n",
       " 'Overstride_12.mp4',\n",
       " 'Overstride_7.mp4',\n",
       " 'Overstride_6.mp4',\n",
       " 'Overstride_4.mp4',\n",
       " 'Overstride_5.mp4',\n",
       " 'Overstride_2.mp4',\n",
       " 'Overstride_3.mp4',\n",
       " 'Overstride_8.mp4',\n",
       " 'Overstride_9.mp4']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1715063415.273276       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063423.528371       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063432.063810       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063440.198645       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063449.497605       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063459.463148       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063468.746910       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063476.553606       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063485.012969       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063493.722346       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063505.091825       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063513.057291       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063521.082400       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063530.356011       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063538.214711       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063548.641673       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SolutionBase.close of <mediapipe.python.solutions.holistic.Holistic object at 0x2d215cb50>>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop through video files for each .mp4 file\n",
    "for videomp4 in video_files:\n",
    "    # Construct the full path to the video file\n",
    "    vid_file = os.path.join(source_path, videomp4)\n",
    "    \n",
    "    # Create the directory to store the numpy arrays for this video\n",
    "    video_dir = os.path.join(DATA_PATH, videomp4[:-4])  # Remove the '.mp4' extension from the video file name\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    \n",
    "    # Create an opencv object from our video\n",
    "    cap = cv2.VideoCapture(vid_file)\n",
    "    \n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Timestep\n",
    "    timestep = 0\n",
    "\n",
    "    # Set mediapipe model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "            # Once feed is at its last frame, there will be no ret value, break loop and move to next video\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Make detections using previously defined function to detect keypoints\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "            # Export Keypoints\n",
    "            keypoints = extract_keypoints(results) # Extract keypoints from the results object\n",
    "\n",
    "            # Save numpy in new folder\n",
    "            timestep+=1\n",
    "            npy_path = os.path.join(video_dir, str(timestep) + '.npy') # Create a path to save the numpy array to be generated below\n",
    "            np.save(npy_path, keypoints) # Save the keypoints data in a numpy array onto the path defined\n",
    "            \n",
    "            # Show to screen with defined function to draw points and lines\n",
    "            draw_landmarks(image, results)\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            # Break loop if 'q' is pressed\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    # Release video capture and destroy OpenCV windows\n",
    "    cap.release()\n",
    "    if cv2.getWindowProperty('OpenCV Feed', cv2.WND_PROP_VISIBLE) >= 1:\n",
    "        cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)  # Added this\n",
    "    cap.release\n",
    "\n",
    "holistic.close\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Overstride Flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for exported data (numpy arrays)\n",
    "DATA_PATH = '../datasets/overstride_npy'\n",
    "\n",
    "# Create video extraction source path \n",
    "source_path = '../datasets/overstride_augment/'\n",
    "\n",
    "# Get a list of all video files in the source folder\n",
    "video_files = [f for f in os.listdir(source_path) if f.endswith('.mp4')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1715063561.452600       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063569.899287       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063580.022839       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063588.191071       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063597.410476       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063605.790377       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063613.701329       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063621.488399       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063632.732314       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063643.204065       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063652.607307       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063661.979691       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063670.458550       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063679.206334       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063687.633916       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063697.039363       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SolutionBase.close of <mediapipe.python.solutions.holistic.Holistic object at 0x2d0fc86d0>>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop through video files for each .mp4 file\n",
    "for videomp4 in video_files:\n",
    "    # Construct the full path to the video file\n",
    "    vid_file = os.path.join(source_path, videomp4)\n",
    "    \n",
    "    # Create the directory to store the numpy arrays for this video\n",
    "    video_dir = os.path.join(DATA_PATH, videomp4[:-4])  # Remove the '.mp4' extension from the video file name\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    \n",
    "    # Create an opencv object from our video\n",
    "    cap = cv2.VideoCapture(vid_file)\n",
    "    \n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Timestep\n",
    "    timestep = 0\n",
    "\n",
    "    # Set mediapipe model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "            # Once feed is at its last frame, there will be no ret value, break loop and move to next video\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Make detections using previously defined function to detect keypoints\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "            # Export Keypoints\n",
    "            keypoints = extract_keypoints(results) # Extract keypoints from the results object\n",
    "\n",
    "            # Save numpy in new folder\n",
    "            timestep+=1\n",
    "            npy_path = os.path.join(video_dir, str(timestep) + '.npy') # Create a path to save the numpy array to be generated below\n",
    "            np.save(npy_path, keypoints) # Save the keypoints data in a numpy array onto the path defined\n",
    "            \n",
    "            # Show to screen with defined function to draw points and lines\n",
    "            draw_landmarks(image, results)\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            # Break loop if 'q' is pressed\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    # Release video capture and destroy OpenCV windows\n",
    "    cap.release()\n",
    "    if cv2.getWindowProperty('OpenCV Feed', cv2.WND_PROP_VISIBLE) >= 1:\n",
    "        cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)  # Added this\n",
    "    cap.release\n",
    "\n",
    "holistic.close\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 No Overstride Flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for exported data (numpy arrays)\n",
    "DATA_PATH = '../datasets/no_overstride_npy'\n",
    "\n",
    "# Create video extraction source path \n",
    "source_path = '../datasets/no_overstride_augment/'\n",
    "\n",
    "# Get a list of all video files in the source folder\n",
    "video_files = [f for f in os.listdir(source_path) if f.endswith('.mp4')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1715063711.143049       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063716.794014       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063725.149623       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063733.674335       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063744.155393       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063755.112609       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063763.671771       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063775.172110       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063785.252647       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063788.278589       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063796.022628       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063801.054860       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063803.938982       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063812.416282       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1715063822.182356       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SolutionBase.close of <mediapipe.python.solutions.holistic.Holistic object at 0x2df8edaf0>>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop through video files for each .mp4 file\n",
    "for videomp4 in video_files:\n",
    "    # Construct the full path to the video file\n",
    "    vid_file = os.path.join(source_path, videomp4)\n",
    "    \n",
    "    # Create the directory to store the numpy arrays for this video\n",
    "    video_dir = os.path.join(DATA_PATH, videomp4[:-4])  # Remove the '.mp4' extension from the video file name\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    \n",
    "    # Create an opencv object from our video\n",
    "    cap = cv2.VideoCapture(vid_file)\n",
    "    \n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Timestep\n",
    "    timestep = 0\n",
    "\n",
    "    # Set mediapipe model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "            # Once feed is at its last frame, there will be no ret value, break loop and move to next video\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Make detections using previously defined function to detect keypoints\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "            # Export Keypoints\n",
    "            keypoints = extract_keypoints(results) # Extract keypoints from the results object\n",
    "\n",
    "            # Save numpy in new folder\n",
    "            timestep+=1\n",
    "            npy_path = os.path.join(video_dir, str(timestep) + '.npy') # Create a path to save the numpy array to be generated below\n",
    "            np.save(npy_path, keypoints) # Save the keypoints data in a numpy array onto the path defined\n",
    "            \n",
    "            # Show to screen with defined function to draw points and lines\n",
    "            draw_landmarks(image, results)\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            # Break loop if 'q' is pressed\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    # Release video capture and destroy OpenCV windows\n",
    "    cap.release()\n",
    "    if cv2.getWindowProperty('OpenCV Feed', cv2.WND_PROP_VISIBLE) >= 1:\n",
    "        cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)  # Added this\n",
    "    cap.release\n",
    "\n",
    "holistic.close\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each frame's keypoint coordinates have been extracted into a singular numpy array. With all the frames from a video saved in their individual folders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we will observe the differences between angles and positions of different keypoints across runners in both classes. This will help us identify key features that will be included in the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
